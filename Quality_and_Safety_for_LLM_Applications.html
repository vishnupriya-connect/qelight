<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Quality and Safety for LLM Applications - Course Syllabus</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" />
    <style>
      body {
        font-family: Arial, sans-serif;
      }
      .header {
        background-color: #003366;
        color: white;
        padding: 1.5rem;
        text-align: center;
      }
      .container {
        margin-top: 20px;
      }
      .section-title {
        font-size: 1.25rem;
        font-weight: bold;
        color: #003366;
      }
      .list-group-item {
        font-size: 1.1rem;
      }
    </style>
  </head>
  <body>
    <!-- Header Section -->
    <header class="header">
      <h1>Quality and Safety for LLM Applications</h1>
      <p>Course Syllabus</p>
    </header>

    <!-- Course Details Section -->
    <section class="container mt-4">
      <h2 class="section-title">What You'll Learn</h2>
      <ul class="list-group mb-4">
        <li class="list-group-item">Monitor and enhance security measures to protect your LLM applications over time.</li>
        <li class="list-group-item">Detect and prevent critical security threats like hallucinations, jailbreaks, and data leakage.</li>
        <li class="list-group-item">Explore real-world scenarios to better prepare for potential risks and vulnerabilities.</li>
      </ul>

      <h2 class="section-title">About This Course</h2>
      <p>
        This course focuses on addressing and monitoring safety and quality concerns specific to LLM applications, which present unique security
        challenges. Youâ€™ll learn best practices, metrics, and hands-on techniques for improving the safety and quality of your applications.
      </p>
      <ul class="list-group mb-4">
        <li class="list-group-item">Identify hallucinations with methods like SelfCheckGPT.</li>
        <li class="list-group-item">Detect jailbreaks and mitigate manipulative prompts using sentiment analysis and toxicity detection.</li>
        <li class="list-group-item">Identify and prevent data leakage using entity recognition and vector similarity.</li>
        <li class="list-group-item">Build a monitoring system to continuously evaluate the safety and security of your application.</li>
      </ul>
      <p>
        By the end of this course, you will be well-equipped to identify and address common security concerns in LLM-based applications, customizing
        your safety evaluation tools to suit the LLM in use.
      </p>

      <h2 class="section-title">Course Outline</h2>
      <ul class="list-group">
        <li class="list-group-item">
          <strong>Introduction</strong><br />
          Introduction to safety and quality considerations in LLM applications.
        </li>
        <li class="list-group-item">
          <strong>Overview</strong><br />
          General overview of monitoring systems and best practices for secure LLM applications.
        </li>
        <li class="list-group-item">
          <strong>Hallucinations</strong><br />
          Techniques to detect and mitigate hallucinations using methods like SelfCheckGPT.
        </li>
        <li class="list-group-item">
          <strong>Data Leakage</strong><br />
          Identifying and preventing data leakage with entity recognition and vector similarity analysis.
        </li>
        <li class="list-group-item">
          <strong>Refusals and Prompt Injections</strong><br />
          Managing prompt injections and refusals through sentiment analysis and toxicity detection.
        </li>
        <li class="list-group-item">
          <strong>Passive and Active Monitoring</strong><br />
          Implementing both passive and active monitoring systems to maintain application security over time.
        </li>
        <li class="list-group-item">
          <strong>Conclusion</strong><br />
          Recap of safety practices and key takeaways for securing LLM applications.
        </li>
      </ul>

      <h2 class="section-title mt-4">Who Should Join?</h2>
      <p>
        This course is ideal for anyone with basic Python knowledge who is interested in mitigating issues like hallucinations, prompt injections, and
        toxic outputs in LLM applications.
      </p>
    </section>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.6/dist/umd/popper.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.min.js"></script>
  </body>
</html>
