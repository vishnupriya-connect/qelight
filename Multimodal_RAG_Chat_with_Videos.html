<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Multimodal RAG: Chat with Videos - Course Syllabus</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" />
    <style>
      body {
        font-family: Arial, sans-serif;
      }
      .header {
        background-color: #003366;
        color: white;
        padding: 1.5rem;
        text-align: center;
      }
      .container {
        margin-top: 20px;
      }
      .section-title {
        font-size: 1.25rem;
        font-weight: bold;
        color: #003366;
      }
      .list-group-item {
        font-size: 1.1rem;
      }
    </style>
  </head>
  <body>
    <!-- Header Section -->
    <header class="header">
      <h1>Multimodal RAG: Chat with Videos</h1>
      <p>Course Syllabus</p>
    </header>

    <!-- Course Details Section -->
    <section class="container mt-4">
      <h2 class="section-title">What You'll Learn</h2>
      <ul class="list-group mb-4">
        <li class="list-group-item">Create an advanced question-answering system that interacts with multimodal data, including video.</li>
        <li class="list-group-item">Understand the concept of multimodal semantic space and its significance in AI.</li>
        <li class="list-group-item">Differentiate between traditional RAG and multimodal RAG systems, focusing on model integration complexities.</li>
      </ul>

      <h2 class="section-title">About This Course</h2>
      <p>
        This course, developed in collaboration with Intel, guides you in building an interactive system to query and understand video content through
        multimodal AI. Learn to implement a multimodal RAG system, utilizing multimodal embedding models for embedding images and captions in a
        semantic space, and leverage this setup for retrieval using text prompts.
      </p>

      <h2 class="section-title">Key Technologies and Concepts</h2>
      <ul class="list-group mb-4">
        <li class="list-group-item">
          <strong>Multimodal Embedding Models</strong>: BridgeTower for creating joint embeddings of image-caption pairs.
        </li>
        <li class="list-group-item"><strong>Video Processing</strong>: Whisper model for transcription, LVLMs for captioning.</li>
        <li class="list-group-item"><strong>Vector Stores</strong>: LanceDB for efficient storage and retrieval of high-dimensional vectors.</li>
        <li class="list-group-item"><strong>Retrieval Systems</strong>: LangChain for building a retrieval pipeline.</li>
        <li class="list-group-item"><strong>Large Vision Language Models (LVLMs)</strong>: LLaVA 1.5 for advanced visual-textual understanding.</li>
        <li class="list-group-item">
          <strong>APIs and Cloud Infrastructure</strong>: PredictionGuard APIs, Intel Gaudi AI accelerators, Intel Developer Cloud.
        </li>
      </ul>

      <h2 class="section-title">Hands-on Project</h2>
      <p>Throughout the course, youâ€™ll build a complete multimodal RAG system that:</p>
      <ul class="list-group mb-4">
        <li class="list-group-item">Processes and embeds video content (frames, transcripts, and captions).</li>
        <li class="list-group-item">Stores multimodal data in a vector database.</li>
        <li class="list-group-item">Retrieves relevant video segments based on text queries.</li>
        <li class="list-group-item">Generates contextual responses using LVLMs.</li>
        <li class="list-group-item">Maintains multi-turn conversations about video content.</li>
      </ul>

      <h2 class="section-title">Course Outline</h2>
      <ul class="list-group">
        <li class="list-group-item"><strong>Introduction</strong>: Overview of multimodal RAG systems and interactive video chat capabilities.</li>
        <li class="list-group-item">
          <strong>Interactive Demo and Multimodal RAG System Architecture</strong>: Introduction to the system architecture with a Gradio app demo.
        </li>
        <li class="list-group-item">
          <strong>Multimodal Embeddings</strong>: Understanding and creating joint embeddings with the BridgeTower model.
        </li>
        <li class="list-group-item">
          <strong>Preprocessing Videos for Multimodal RAG</strong>: Extracting frames, generating transcripts with Whisper, and captioning using
          LVLMs.
        </li>
        <li class="list-group-item"><strong>Multimodal Retrieval from Vector Stores</strong>: Implementing retrieval using LanceDB and LangChain.</li>
        <li class="list-group-item">
          <strong>Large Vision - Language Models (LVLMs)</strong>: Understanding LVLM architecture and implementing visual question answering.
        </li>
        <li class="list-group-item">
          <strong>Multimodal RAG with Multimodal Langchain</strong>: Building a multimodal RAG pipeline using LangChain.
        </li>
        <li class="list-group-item"><strong>Conclusion</strong>: Summary and future directions.</li>
      </ul>

      <h2 class="section-title mt-4">Who Should Join?</h2>
      <p>
        This course is for anyone with intermediate Python programming knowledge, familiarity with machine learning concepts and deep learning
        frameworks, and a basic understanding of natural language processing and computer vision.
      </p>
    </section>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.6/dist/umd/popper.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.min.js"></script>
  </body>
</html>
