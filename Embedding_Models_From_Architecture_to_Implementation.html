<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Embedding Models: From Architecture to Implementation - Course Syllabus</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" />
    <style>
      body {
        font-family: Arial, sans-serif;
      }
      .header {
        background-color: #003366;
        color: white;
        padding: 1.5rem;
        text-align: center;
      }
      .container {
        margin-top: 20px;
      }
      .section-title {
        font-size: 1.25rem;
        font-weight: bold;
        color: #003366;
      }
      .list-group-item {
        font-size: 1.1rem;
      }
    </style>
  </head>
  <body>
    <!-- Header Section -->
    <header class="header">
      <h1>Embedding Models: From Architecture to Implementation</h1>
      <p>Course Syllabus</p>
    </header>

    <!-- Course Details Section -->
    <section class="container mt-4">
      <h2 class="section-title">What You'll Learn</h2>
      <ul class="list-group mb-4">
        <li class="list-group-item">Gain an in-depth understanding of embedding model architecture and learn how to train and utilize them.</li>
        <li class="list-group-item">Implement various embedding models like Word2Vec and BERT for semantic search.</li>
        <li class="list-group-item">Develop and train dual encoder models using contrastive loss to improve question-answer retrieval accuracy.</li>
      </ul>

      <h2 class="section-title">About This Course</h2>
      <p>
        In this course, you will dive into embedding models’ architectures and applications in AI, specifically for capturing semantic meaning in
        words and sentences. You’ll progress from word to sentence embeddings and learn to build a simple dual encoder model. This hands-on course
        provides insights into the technical aspects and effective usage of embedding models.
      </p>
      <ul class="list-group mb-4">
        <li class="list-group-item">
          <strong>Embedding Types</strong>: Learn about word, sentence, and cross-encoder embeddings used in Retrieval Augmented Generation (RAG).
        </li>
        <li class="list-group-item">
          <strong>Transformer Models for Semantic Search</strong>: Explore BERT’s role and usage in advanced search systems.
        </li>
        <li class="list-group-item">
          <strong>Dual Encoder Training with Contrastive Loss</strong>: Train separate encoders for questions and responses to improve retrieval
          accuracy.
        </li>
        <li class="list-group-item">
          <strong>Implementing Embeddings in RAG</strong>: Utilize separate encoders in a RAG pipeline and assess their retrieval impact.
        </li>
      </ul>

      <h2 class="section-title">Course Outline</h2>
      <ul class="list-group">
        <li class="list-group-item">
          <strong>Introduction</strong><br />
          Overview of embedding models and their significance in AI applications.
        </li>
        <li class="list-group-item">
          <strong>Introduction to embedding models</strong><br />
          Basics of embedding models, covering word and sentence embeddings.
        </li>
        <li class="list-group-item">
          <strong>Contextualized token embeddings</strong><br />
          Exploration of token embeddings and how context is used to enhance their meaning.
        </li>
        <li class="list-group-item">
          <strong>Token vs. sentence embedding</strong><br />
          Comparison of token and sentence embeddings with practical examples.
        </li>
        <li class="list-group-item">
          <strong>Training a dual encoder</strong><br />
          Hands-on training of a dual encoder model using contrastive loss.
        </li>
        <li class="list-group-item">
          <strong>Using embeddings in RAG</strong><br />
          Application of embeddings within a RAG pipeline for enhanced retrieval.
        </li>
        <li class="list-group-item">
          <strong>Conclusion</strong><br />
          Summary of the course concepts and next steps for embedding model application.
        </li>
        <li class="list-group-item">
          <strong>Quiz – Test your knowledge</strong><br />
          Assess your understanding with a quiz covering course content.
        </li>
        <li class="list-group-item">
          <strong>Appendix – Tips and Help</strong><br />
          Additional resources and code examples to assist with course concepts.
        </li>
      </ul>

      <h2 class="section-title mt-4">Who Should Join?</h2>
      <p>
        This course is ideal for data scientists, machine learning engineers, and NLP enthusiasts who want to learn about embedding models for
        semantic retrieval systems. Basic knowledge of Python is recommended.
      </p>
    </section>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.6/dist/umd/popper.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.min.js"></script>
  </body>
</html>
