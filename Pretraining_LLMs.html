<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Pretraining LLMs - Course Syllabus</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" />
    <style>
      body {
        font-family: Arial, sans-serif;
      }
      .header {
        background-color: #003366;
        color: white;
        padding: 1.5rem;
        text-align: center;
      }
      .container {
        margin-top: 20px;
      }
      .section-title {
        font-size: 1.25rem;
        font-weight: bold;
        color: #003366;
      }
      .list-group-item {
        font-size: 1.1rem;
      }
    </style>
  </head>
  <body>
    <!-- Header Section -->
    <header class="header">
      <h1>Pretraining LLMs</h1>
      <p>Course Syllabus</p>
    </header>

    <!-- Course Details Section -->
    <section class="container mt-4">
      <h2 class="section-title">What You'll Learn</h2>
      <ul class="list-group mb-4">
        <li class="list-group-item">Understand all steps for pretraining an LLM, from data preparation to model performance assessment.</li>
        <li class="list-group-item">Explore model configuration options, including modifying architectures and weight initialization.</li>
        <li class="list-group-item">Learn innovative techniques like Depth Upscaling to reduce training costs significantly.</li>
      </ul>

      <h2 class="section-title">About This Course</h2>
      <p>
        This course delves into the initial step of training large language models through pretraining, covering essential steps, cost considerations,
        and effective strategies for using existing open-source models to reduce expenses.
      </p>
      <ul class="list-group mb-4">
        <li class="list-group-item">
          <strong>Optimal Scenarios</strong>: Understand when pretraining is the best choice for enhancing model performance.
        </li>
        <li class="list-group-item">
          <strong>Dataset Creation</strong>: Learn to build a high-quality training dataset using web text and existing resources.
        </li>
        <li class="list-group-item">
          <strong>Data Preparation</strong>: Prepare your dataset for training, formatted for use with the Hugging Face library.
        </li>
        <li class="list-group-item">
          <strong>Model Configuration</strong>: Configure model initialization and explore choices impacting pretraining speed.
        </li>
        <li class="list-group-item">
          <strong>Execution</strong>: Set up and run a training process, guiding you through the actual training process.
        </li>
        <li class="list-group-item">
          <strong>Performance Evaluation</strong>: Assess trained models using benchmark tasks and standard evaluation strategies.
        </li>
      </ul>

      <h2 class="section-title">Course Outline</h2>
      <ul class="list-group">
        <li class="list-group-item">
          <strong>Introduction</strong><br />
          Overview of the course, covering the goals and structure of LLM pretraining.
        </li>
        <li class="list-group-item">
          <strong>Why Pre-training</strong><br />
          Understanding the purpose and benefits of pretraining for language models.
        </li>
        <li class="list-group-item">
          <strong>Data Preparation</strong><br />
          Techniques for preparing datasets to ensure quality training data for LLMs.
        </li>
        <li class="list-group-item">
          <strong>Packaging Data for Pretraining</strong><br />
          Formatting data for use in the pretraining process with the Hugging Face library.
        </li>
        <li class="list-group-item">
          <strong>Model Initialization</strong><br />
          Configuring model initialization options to optimize the pretraining process.
        </li>
        <li class="list-group-item">
          <strong>Training in Action</strong><br />
          Practical application of training, including setup and execution details.
        </li>
        <li class="list-group-item">
          <strong>Evaluation</strong><br />
          Methods and metrics for evaluating pre-trained LLM performance.
        </li>
        <li class="list-group-item">
          <strong>Conclusion</strong><br />
          Summary of key takeaways and next steps for implementing pretraining in projects.
        </li>
      </ul>

      <h2 class="section-title mt-4">Who Should Join?</h2>
      <p>
        This course is ideal for AI enthusiasts, data scientists, and ML engineers seeking to understand pretraining for LLMs. Basic Python and LLM
        knowledge are recommended.
      </p>
    </section>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.6/dist/umd/popper.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.min.js"></script>
  </body>
</html>
