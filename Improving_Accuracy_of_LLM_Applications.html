<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Improving Accuracy of LLM Applications - Course Syllabus</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" />
    <style>
      body {
        font-family: Arial, sans-serif;
      }
      .header {
        background-color: #003366;
        color: white;
        padding: 1.5rem;
        text-align: center;
      }
      .container {
        margin-top: 20px;
      }
      .section-title {
        font-size: 1.25rem;
        font-weight: bold;
        color: #003366;
      }
      .list-group-item {
        font-size: 1.1rem;
      }
    </style>
  </head>
  <body>
    <!-- Header Section -->
    <header class="header">
      <h1>Improving Accuracy of LLM Applications</h1>
      <p>Course Syllabus</p>
    </header>

    <!-- Course Details Section -->
    <section class="container mt-4">
      <h2 class="section-title">What You'll Learn</h2>
      <ul class="list-group mb-4">
        <li class="list-group-item">
          Develop steps to improve model reliability and accuracy through evaluation, prompting, self-reflection, and fine-tuning.
        </li>
        <li class="list-group-item">Use memory tuning to embed facts in the model, enhancing performance and reducing hallucinations.</li>
        <li class="list-group-item">Build an LLM application using the Llama 3-8b model to convert text into SQL queries with a custom schema.</li>
      </ul>

      <h2 class="section-title">About This Course</h2>
      <p>
        In this course, led by Sharon Zhou (Lamini) and Amit Sangani (Meta), you'll learn structured techniques to increase the accuracy of LLM
        applications. The course covers prompt engineering, evaluation metrics, and fine-tuning approaches to reduce hallucinations and improve
        factuality. By the end, you’ll have built and fine-tuned an SQL agent using Llama’s models to meet specific accuracy criteria.
      </p>
      <ul class="list-group mb-4">
        <li class="list-group-item">
          <strong>Text-to-SQL Agent</strong>: Build an agent that generates SQL queries, incorporating error simulation for evaluation.
        </li>
        <li class="list-group-item">
          <strong>Performance Evaluation Framework</strong>: Create an evaluation system with metrics to measure performance effectively.
        </li>
        <li class="list-group-item">
          <strong>Instruction and Memory Fine-Tuning</strong>: Use techniques like LoRA and memory fine-tuning to embed factual accuracy in the model
          and improve its responsiveness.
        </li>
        <li class="list-group-item">
          <strong>Advanced PEFT Techniques</strong>: Explore methods like LoRA and MoME to optimize training time and accuracy.
        </li>
        <li class="list-group-item">
          <strong>Iterative Data Generation and Fine-Tuning</strong>: Gain practical skills in generating training data, creating data variations, and
          filtering to increase model accuracy iteratively.
        </li>
      </ul>

      <h2 class="section-title">Course Outline</h2>
      <ul class="list-group">
        <li class="list-group-item">
          <strong>Introduction</strong><br />
          Introduction to enhancing the accuracy and reliability of LLM applications.
        </li>
        <li class="list-group-item">
          <strong>Overview</strong><br />
          Overview of methods and techniques for consistent, accurate model performance.
        </li>
        <li class="list-group-item">
          <strong>Create an SQL Agent</strong><br />
          Build a text-to-SQL agent and evaluate performance through error simulations.
        </li>
        <li class="list-group-item">
          <strong>Create an Evaluation</strong><br />
          Develop an evaluation framework with criteria and scoring for model accuracy.
        </li>
        <li class="list-group-item">
          <strong>Fine-tuning, PEFT, & Memory Tuning</strong><br />
          Introduction to LoRA, MoME, and memory tuning to embed facts in model weights.
        </li>
        <li class="list-group-item">
          <strong>Generate Data & Finetune</strong><br />
          Steps to generate data, create variations, and fine-tune the model iteratively.
        </li>
        <li class="list-group-item">
          <strong>Conclusion</strong><br />
          Summarization of key concepts and best practices for accurate LLM applications.
        </li>
      </ul>

      <h2 class="section-title mt-4">Who Should Join?</h2>
      <p>
        This course is ideal for individuals with intermediate Python knowledge and familiarity with large language models who want to improve the
        factual accuracy of LLM applications.
      </p>
    </section>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.6/dist/umd/popper.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.min.js"></script>
  </body>
</html>
